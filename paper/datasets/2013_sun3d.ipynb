{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUN3D: A Database of Big Spaces Reconstructed using SfM and Object Labels\n",
    "\n",
    "[[WEB](http://sun3d.cs.princeton.edu/) | [PDF](https://vision.princeton.edu/projects/2013/SUN3D/paper.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "- Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces.\n",
    "\n",
    "\n",
    "- Combine video labeling and structure from motion (SfM):\n",
    "    1. introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another\n",
    "    2. use the object labels to fix errors in the reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Many 3D datasets inherit many of the limitations of traditional 2D datasets: they contain **a sample of** views from the world, but the **physical relationship** between these views and the structure of the space containing them is mostly **missing**.\n",
    "\n",
    "The items in SUN3D are full 3D models with semantics: **RGB-D images, camera poses, object segmentations, point clouds** registered into a global coordinate frame.\n",
    "\n",
    "**_NOTICEABLE_**: <br>\n",
    "This database **requires camera poses**, but estimating them reliably for large space from an RGB-D video is a difficult problem. And despite recent progress in RGB-D structure-from-motion (SfM), existing automatic reconstruction methods are not reliable enough for our purposes.Additionally, we desire a semantic segmentation, but **labeling every frame in a full video is a painstaking task** - for this reason, existing RGB-D video databases have semantic annotations only for a sparse subset of their frames.\n",
    "\n",
    "**_COMMENT_**: <br>\n",
    "For a our current (2019.08.22) dataset, the camera poses are known and can be provided as groundtruths. And since the user can change the position of the camera in the simulation environment (i.e. Gazebo), capturing a long video may not be required. Additionaly, SUN3D provides **pre-captured** videos.\n",
    "\n",
    "### 1.1 Related Work\n",
    "**_COMMENT_**:\n",
    "1. SUN3D contains full apartment scenes, which also exist in Matterport3D.\n",
    "2. Both SUN3D and mentioned dataset provide some way for object recognition, but our current work (2019.08.22) does not provide such tools nor data groundtruths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
