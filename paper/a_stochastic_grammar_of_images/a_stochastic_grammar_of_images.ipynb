{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Stochastic Grammar of Images\n",
    "\n",
    "Original [link](http://www.stat.ucla.edu/~sczhu/papers/Grammar_quest.pdf).\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [1 Introduction](#1-introduection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.2 Objectives\n",
    "Given an input image, the image parsing task constructs a most probable parse graph on-the-fly as the output interpretation and this parse graph is a subgraph of the And-Or graph (AOG) after making choices on the Or-nodes.\n",
    "\n",
    "Objectives:\n",
    "1. A common framework (that is AOG) for visual knowledge representation and object categorization.\n",
    "2. Scalable and recursive top-down/bottom-up computation.\n",
    "3. Small sample learning and generalization.\n",
    "4. Mapping the visual vocabulary to fill the [semantic gap](https://en.wikipedia.org/wiki/Semantic_gap) between symbols and pixels.\n",
    "\n",
    "\n",
    "Computational procedure:\n",
    "1. Bottom-up detecting and binding using a cascade of features.\n",
    "2. Top-down on-line template composition and matching.\n",
    "\n",
    "![fig2](./fig/fig2.png)\n",
    "Figure 2: Illustrating the recursive bottom-up / top-down computation processure in image parsing. The detection of rectangles (in red) instantiates some non-terminal nodes shown as upward arrows. They in turn activate graph grammar rules for grouping larger structures in nodes A, B, C respectively. These rules generate top-down prediction of rectangles (in blue). The redictions are validated from the image under the Bayesian posterior probability.\n",
    "\n",
    "### 1.3 Overview of the Image Grammar\n",
    "![fig3](./fig/fig3.png)\n",
    "Figure 3: Illustrating the And-Or graph representation. (a) An And-Or graph embodies the grammar productions rules and contexts. It contains many parse graphs, one of which is shown in bold arrows. (b) and (c) are two distinct parse graphs by selecting the switches at related Or-nodes. (d) and (e) are two graphical configurations produced by the two parse graphs respectively. The links of these configurations are inherited from the And-Or graph relations.\n",
    "\n",
    "#### 1.3.1 Representation\n",
    "1. **And-node**: represents a decomposition of an entity into its parts, for example (grammar rules) $A \\rightarrow BCD$, $H \\rightarrow NO$.\n",
    "2. **Or-node**: acts as \"switches\" for alternative sub-structures and stands for labels of classification at various levels, for example $B \\rightarrow E|F$, $C \\rightarrow G|H|I$.\n",
    "3. **Parse graph**: derived from the And-Or graph by selecting the swithes or classfication labels at related Or-nodes, the part shared by two node may have different instances. For example, node $I$ is a child of both nodes $C$ and $D$. Thus we have two instances for node 9.\n",
    "4. **Configuration**: a planar attribute graph formed by linking the open bonds of the primitives in the image plane. When the parse graph collapses, it produces a planar configuration. A configuration inherits the relations from its ancestor nodes, and can be viewed as a Markov networks (or deformable templates) with reconfigurable neighborhood.\n",
    "5. **Visual vocabulary**: due to **scaling property**, the terminal nodes could appear at all levels of the And-Or graph. Each terminal node takes instances from a certain set called a dictionary and contains image patches of various complexities. The elements in the set may be indexed by variables such as its type, geometric transformations, deformations, appearance changes etc. Each patch is augmented with anchor points and open bond to connect with other patches.\n",
    "6. **Language of grammar**: the set of all possible valid configurations produced by the grammar. In stochastic grammar, each configuration is associated with a probability. As the And-Or graph is directed and recursive, the sub-graph underneath any node $A$ can be considered a sub-grammar for the concept represented by node $A$. Thus a sub-language for node $A$ is the set of all valid configurations produced by the And-Or graph rooted at $A$. For example, if $A$ is an object category, say a car, then this sub-language defines all the valid configurations of car. In an exiting case, the sub-language of a terminal node contains only the atomic configurations and thus is called a **dictionary**.\n",
    "\n",
    "#### 1.3.2 Dataset and Learning\n",
    "This paper proposes a semi-automatic way to learn the image grammar: start with a supervised learning with manually annotated images and objects to produce the parse graphs, then use this dataset to initiate the process and then shift to weakly supervised learning.\n",
    "\n",
    "The learning steps are guided by a **minimax extropy learning scheme** and **maximum likeihood estimation**:\n",
    "1. Learning the probabilities at the Or-node so that the configurations generated account for the natural co-occurrence frequency.\n",
    "2. Learning and pursuing the Markov models on the **horizontal** links and relations to account for the spatial relations, as well as consistency of appearance between nodes in the And-Or graphs. This is similar to the learning of **Markov random fields**, except that we are dealing with a dynamic graphical configuration instead of a fixed neighborhood.\n",
    "3. Learning the And-Or graph structures and dictionaries. The terminal nodes are learned through **clustering** and the non-terminal nodes are learned through **binding**.\n",
    "\n",
    "![fig5](./fig/fig5.png)\n",
    "Figure 5: Two examples of the parse trees (cat and car) in the Lotus Hill Research Institute image corpus.\n",
    "\n",
    "The proposed stochastic context sensitive grammar (SCSG) combines the reconfigurability of stochastic context free grammar (SCFG) with the contextual constraints of graphical (MRF) models, and has the following propertiesï¼š\n",
    "1. Compositional power for representing large intra-class structural variations. The grammar can generate a huge number of configurations (i.e. its language) for scenes and objects by composing a relatively much smaller vocabulary. The language of the grammar is the set of all valid configurations of a category.\n",
    "2. Recursive structures for scalable computing. We only need to write general top-down and bottom-up functions for a common And-Or node, and re-use the code for all nodes in the And-Or graph.\n",
    "3. Small sample for effective learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
